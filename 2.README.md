# Overview

- Additional explanations and plots for a DeepMind-style RL model.
- Hyperparameter tuning performed using Optuna.
- Further details on the reward function are provided.

# Additional Plots

![state_ASD_DM](https://raw.githubusercontent.com/Arush-Pimpalkar/RL_Arush/main/plots/state_ASD_DM.png)

![action_ASD_DM](https://raw.githubusercontent.com/Arush-Pimpalkar/RL_Arush/main/plots/action_ASD_DM.png)

![noise_action_DM](https://raw.githubusercontent.com/Arush-Pimpalkar/RL_Arush/main/plots/noise_vs_action.png)


# Reward Function Explanation

![Base Reward Function](https://raw.githubusercontent.com/Arush-Pimpalkar/RL_Arush/main/plots/base_reward_function.png)


The angle term ($\theta$) is assigned unit weight to define the primary regulation objective. The angular velocity term ($\dot{\theta}$) is downweighted to account for its larger numerical scale and to provide appropriate damping, consistent with the pendulumâ€™s natural frequency. The control penalty ($u$) is chosen several orders of magnitude smaller to regularize the policy without inhibiting stabilization, matching the torque scale required to counteract gravity. Together, these weights approximate a quadratic energy-based cost, closely related to LQR and classical damping design, ensuring physically meaningful and stable behavior.


## Reward Function with High-Frequency Penalty

The reward function used in this model:

![DM reward function](https://raw.githubusercontent.com/Arush-Pimpalkar/RL_Arush/main/plots/DM_reward_function.jpeg)

Integrating the high-frequency component with the base reward function in a multiplicative manner led to instability during training and rollouts. Therefore, an additive formulation is used.