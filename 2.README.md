# Overview

- Additional explanations and plots for two DeepMind-style RL models.
- Hyperparameter tuning performed using Optuna.
- Further details on the reward function are provided.

# Models - State penalty DM and Action penalty DM
## Action DM: 
This model penalizes high frequency action (torque). It works by passing actions through a LPF filter, and extracts the high frequency difference. The energy is calculated, sigmoided, and integrated in the reward function.

## State DM: 
This models also uses the same approach, but applied to the state (angle). Same energy and sigmoid implementation. 

Models and results are also uploaded to [Neptune](https://app.neptune.ai/rl-gaussian-sac/rl-gaussian-sac/) for result reproducibility. 

# Reward Function Explanation

![Base Reward Function](https://raw.githubusercontent.com/Arush-Pimpalkar/RL_Arush/main/plots/base_reward_function.png)


The angle term ($\theta$) is assigned unit weight to define the primary regulation objective. The angular velocity term ($\dot{\theta}$) is downweighted to account for its larger numerical scale and to provide appropriate damping, consistent with the pendulum’s natural frequency. The control penalty ($u$) is chosen several orders of magnitude smaller to regularize the policy without inhibiting stabilization, matching the torque scale required to counteract gravity. Together, these weights approximate a quadratic energy-based cost, closely related to LQR and classical damping design, ensuring physically meaningful and stable behavior.


## Reward Function with High-Frequency Penalty

The reward function used in this model:

![DM reward function](https://raw.githubusercontent.com/Arush-Pimpalkar/RL_Arush/main/plots/DM_reward_function.png)

Integrating the high-frequency component with the base reward function in a multiplicative manner led to instability during training and rollouts. Therefore, an additive formula is used.

# Additional Plots

![state_ASD_DM](https://raw.githubusercontent.com/Arush-Pimpalkar/RL_Arush/main/plots/state_ASD_DM.png)

![action_ASD_DM](https://raw.githubusercontent.com/Arush-Pimpalkar/RL_Arush/main/plots/action_ASD_DM.png)

![noise_action_DM](https://raw.githubusercontent.com/Arush-Pimpalkar/RL_Arush/main/plots/noise_vs_action.png)

# My Interpretations: 
Action DM performs better because it reduces high-frequency noise more effectively. In this setup, noise is added directly to the torque (action), so the final torque applied to the pendulum is the (model's action) + (Gaussian noise). This simulates external noise from the environment.

Action DM can directly observe the noisy torque being applied. This helps the model learn to reduce and handle the noise more efficiently.

**One may ask why not add noise at the state/position? (answering by interpreting this as a LIGO pendulum)**    

My frame of reference is the pendulum’s bob, which is kept stationary. The noise exists in the external environment and propagates through the "hinge" in the form of a noisy torque. We ultimately want to keep the bob as stationary as possible by counteracting this noise. Adding noise to the state and then training a model would be equivalent to trying to keep the bob and the stationary environment in resonance, which is not the desired case. Although the physics of both approaches would be equivalent for training these models, this approach is effectively the opposite of what is desired for a LIGO pendulum.


# Transient Tests

I performed transient tests by applying external actions to the pendulum. I added a torque of +50Nm at the 5th second and -100Nm at the 8th second. The model is able to compensate and correct its position. This is the Action DM model. 

![ActionDM_external_torque](https://raw.githubusercontent.com/Arush-Pimpalkar/RL_Arush/main/plots/external_torque_ActionDM.png)